{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Каменев Нил БКЛ211\n",
    "## Проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivigtel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import urllib.request\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Собираем корпус текстов\n",
    "url_list = {'http://az.lib.ru/g/gogolx_n_w/text_1832_oryvki_iz_neizv_dramy.shtml': 'Н.В. Гоголь. \"Черновые наброски. Отрывки из неизвестной драмы\" — lib.ru', 'http://az.lib.ru/g/gogolx_n_w/text_0280.shtml': 'Н.В. Гоголь. \"Ночи на вилле\" — lib.ru', 'http://az.lib.ru/g/gogolx_n_w/text_1838_nabroski.shtml': 'Н.В. Гоголь. \"Черновые наброски. Наброски драмы из украинской истории\" — lib.ru', 'http://az.lib.ru/g/gogolx_n_w/text_1835_alfred_var.shtml': 'Н.В. Гоголь. \"Черновые наброски. Варианты\" — lib.ru', 'http://az.lib.ru/g/gogolx_n_w/text_0270.shtml': 'Н.В. Гоголь. \"Повесть о капитане Копейкине\" — lib.ru', 'http://az.lib.ru/g/gogolx_n_w/text_0350.shtml': 'Н.В. Гоголь. \"Мёртвые Души (Том второй)\" — lib.ru'}\n",
    "\n",
    "def get_corpus(url_list):\n",
    "    corpus = []\n",
    "    for key in url_list:\n",
    "        # Загружаем страницу\n",
    "        page = requests.get(key)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # Удаляем все теги и лишние спецсимволы\n",
    "        text_f = re.search('/table>(.*)', str(soup), flags= re.U | re.DOTALL)\n",
    "        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        cleantext = re.sub(cleanr, '', text_f.group(1))\n",
    "        cleaner = re.sub('\\n', ' ', cleantext)\n",
    "        cleaner_2 = re.sub('\\r', ' ', cleaner)\n",
    "        cleaner_3 = re.sub('\\x00', ' ', cleaner_2)\n",
    "        final_cleaner = re.sub('\\xa0', ' ', cleaner_3)\n",
    "        # Разбиваем текст на предложения\n",
    "        sentences = nltk.sent_tokenize(final_cleaner)\n",
    "        # Добавляем каждое предложение в корпус с указанием источника\n",
    "        for s in sentences:\n",
    "            if re.search(r'\\w', s):\n",
    "                corpus.append({'source': key, 'title': url_list[key], 'sentence': s})\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Морфологический анализ текстов\n",
    "corpus = get_corpus(url_list)\n",
    "def analyze_text(corpus):\n",
    "    morph = MorphAnalyzer() #использование морфоанализатора pymorphy2\n",
    "    analyzed_corpus = []\n",
    "    for doc in corpus:\n",
    "        analyzed_doc = []\n",
    "        words = re.findall(r'\\w+', doc['sentence'])\n",
    "        for w in words:\n",
    "            # Анализируем каждое слово и добавляем информацию о лемме и теге\n",
    "            p = morph.parse(w)[0]\n",
    "            analyzed_doc.append({'word': w, 'lemma': p.normal_form, 'tag': p.tag.POS})\n",
    "        analyzed_corpus.append({'source': doc['source'], 'title': doc['title'], 'sentence': doc['sentence'], 'words': analyzed_doc})\n",
    "    return analyzed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите список токенов через пробел: сказали+INFN \"жизнью\" жертвовали\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'http://az.lib.ru/g/gogolx_n_w/text_0270.shtml',\n",
       "  'title': 'Н.В. Гоголь. \"Повесть о капитане Копейкине\" — lib.ru',\n",
       "  'sentence': '\"Пойду в комиссию,  -  говорит  Копейкин,  скажу:  так и так, проливал, в некотором роде, кровь,  относительно  сказать, жизнью жертвовал\".'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция поиска\n",
    "analyzed_corpus = analyze_text(corpus)\n",
    "query = input(\"Введите список токенов через пробел: \") # запрос ввода списка токенов\n",
    "def search(query, analyzed_corpus): # Поиск по корпусу\n",
    "    query_tokens = query.split() # разделение строки на список токенов\n",
    "    result = []\n",
    "    lemmatized_tokens = []\n",
    "    for token in query_tokens:\n",
    "        if '+' in token:\n",
    "            lemmo4ka, postegik = token.split('+')\n",
    "            lemmatized_tokens.append(MorphAnalyzer().parse(lemmo4ka)[0].normal_form.lower()) # лемматизация токенов для запроса леммы с POS-тегом\n",
    "        else:\n",
    "            lemmatized_tokens.append(MorphAnalyzer().parse(token)[0].normal_form.lower()) # лемматизация токенов для запроса леммы\n",
    "    for doc in analyzed_corpus:\n",
    "        for i in range(len(doc['words']) - len(query_tokens) + 1):\n",
    "            match = True\n",
    "            for j in range(len(query_tokens)):\n",
    "                # Проверяем, соответствует ли текущее слово запросу\n",
    "                if '\"' in query_tokens[j]: # словоформа в двойных кавычках (находит только заданную форму)\n",
    "                    if not doc['words'][i+j]['word'].lower() == re.search(r'\\w+', query_tokens[j].lower()).group(0):\n",
    "                        match = False\n",
    "                        break\n",
    "                elif ('\"' not in query_tokens[j]) and ('+' not in query_tokens[j]) and (not query_tokens[j].isupper()): # лемма (находит вхождения во всех формах)\n",
    "                    if not doc['words'][i+j]['lemma'].lower() == lemmatized_tokens[j]:\n",
    "                        match = False\n",
    "                        break\n",
    "                elif '+' in query_tokens[j]: # лемма и POS-тег (находит все формы, отмеченные данным тегом)\n",
    "                    lemmochka, posik = query_tokens[j].split('+')\n",
    "                    if not (doc['words'][i+j]['lemma'].lower() == lemmatized_tokens[j]) or not (doc['words'][i+j]['tag'] == posik):\n",
    "                        match = False\n",
    "                        break\n",
    "                else: # только POS-тег(и)\n",
    "                    if not doc['words'][i+j]['tag'] == query_tokens[j]:\n",
    "                        match = False\n",
    "                        break\n",
    "            if match:\n",
    "                result.append({'source': doc['source'], 'title': doc['title'], 'sentence': doc['sentence']}) # вывод предложений с мета-информацией\n",
    "                break\n",
    "    return result\n",
    "search(query, analyzed_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
